{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "02760436",
   "metadata": {},
   "source": [
    "### core of processor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf529bc2",
   "metadata": {},
   "source": [
    "The core of a processor, also known as a CPU (Central Processing Unit) core, is the part of the processor that performs the actual computations and executes instructions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58eb988",
   "metadata": {},
   "source": [
    "A processor can have multiple cores, which allows it to perform multiple computations at the same time. For example, a quad-core processor has four cores that can work simultaneously, which can result in faster processing and better performance when running multiple applications or performing complex tasks such as video rendering or scientific simulations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b36093",
   "metadata": {},
   "source": [
    "Each core in a processor can handle a single thread of execution at a time. However, some processors support hyper-threading, which allows each core to handle two threads simultaneously, effectively doubling the number of logical processors available."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0c43cb9",
   "metadata": {},
   "source": [
    "Overall, the number of cores in a processor is an important factor to consider when selecting a computer for a particular use case, as it can significantly impact the computer's performance and processing capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4937677a",
   "metadata": {},
   "source": [
    "### core processor vs logical processor "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0701c1b",
   "metadata": {},
   "source": [
    ", a core processor is a physical processing unit that can handle one or more threads of execution simultaneously, while a logical processor is a virtual processing unit created by dividing the resources of a physical core to execute multiple threads of execution simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1375607",
   "metadata": {},
   "source": [
    "The number of cores in a processor determines the maximum number of independent threads of execution that can be executed simultaneously, while the number of logical processors depends on the processor's architecture and support for technologies such as hyper-threading"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe2457a",
   "metadata": {},
   "source": [
    "Get-WmiObject -Class Win32_Processor | Select-Object Name, NumberOfCores, NumberOfLogicalProcessors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e698b147",
   "metadata": {},
   "source": [
    "### Processor architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3610ae55",
   "metadata": {},
   "source": [
    "Processor architecture refers to the underlying hardware design of a computer's CPU (Central Processing Unit). It defines the way in which the CPU is organized, the instruction set it supports, the memory it can address, and the way it communicates with other components of the computer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375389a7",
   "metadata": {},
   "source": [
    "x86: This is the most common processor architecture for Windows-based PCs. It is a CISC architecture and is used by most Intel and AMD processors.\n",
    "\n",
    "x64: This is an extension of the x86 architecture and is used by 64-bit versions of Windows. It allows the CPU to access more memory and provides improved performance for certain types of applications."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "041d995a",
   "metadata": {},
   "source": [
    "This command retrieves information about the processor using WMI (Windows Management Instrumentation) and selects the \"Name\" and \"Architecture\" properties of the \"Win32_Processor\" class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b0587",
   "metadata": {},
   "source": [
    "Get-WmiObject -Class Win32_Processor | Select-Object -Property Name, Architecture\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1676bb6",
   "metadata": {},
   "source": [
    " that the processor is an Intel(R) Core(TM) i3-3240 CPU with an architecture value of 9. Architecture value 9 corresponds to x64 (64-bit) architecture. Therefore, your processor is a 64-bit processor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "785b1dc2",
   "metadata": {},
   "source": [
    " dual-core processor with a base clock speed of 3.4 GHz. It also supports hyper-threading, which allows each core to process two threads simultaneously, effectively providing four virtual cores. The processor has a thermal design power (TDP) of 55 watts and is built on a 22-nanometer manufacturing process. It also features Intel HD Graphics 2500, which is integrated graphics that can support up to three displays and provide improved video playback and graphics performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9449c236",
   "metadata": {},
   "source": [
    "### GPU INFORMATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ecb40f",
   "metadata": {},
   "source": [
    "Using PowerShell: You can use the following PowerShell command to get information about the GPU:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d8d78f5",
   "metadata": {},
   "source": [
    "Get-CimInstance Win32_VideoController | Select-Object Name,AdapterCompatibility,AdapterRAM,DriverVersion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "414c2354",
   "metadata": {},
   "outputs": [],
   "source": [
    "Name                  AdapterCompatibility AdapterRAM DriverVersion\n",
    "----                  -------------------- ---------- -------------\n",
    "NVIDIA GeForce GT 730 NVIDIA               2147483648 27.21.14.5671\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af80099d",
   "metadata": {},
   "source": [
    "AdapterRAM is a property of the Win32_VideoController WMI class that provides information about the amount of video memory (in bytes) installed on the GPU (graphics processing unit)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3ef912",
   "metadata": {},
   "source": [
    "The driver version can be helpful for troubleshooting graphics-related issues, checking for updates to the driver, or verifying that your system meets the minimum requirements for a specific application or game."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8546f7e1",
   "metadata": {},
   "source": [
    "### Dask "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873cf3a4",
   "metadata": {},
   "source": [
    "Dask is a parallel computing framework for Python that enables the parallelization of code across multiple CPU cores, clusters, and even cloud computing services. Dask can handle large datasets that do not fit into memory by creating a graph of computations that can be parallelized and scheduled across multiple workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e9dbfe6",
   "metadata": {},
   "source": [
    "Dask uses task scheduling and lazy evaluation to optimize the use of computational resources. Tasks are scheduled dynamically as data becomes available, and computations are evaluated only when necessary. This approach can significantly improve the performance of data-intensive computations and reduce memory usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e47ddbd",
   "metadata": {},
   "source": [
    "Dask integrates seamlessly with other popular Python libraries such as NumPy, Pandas, and Scikit-learn, allowing users to parallelize existing code with minimal modifications. Dask also provides a dashboard that can be used to monitor the progress of computations and identify performance bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24b2acfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023.3.1\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "print(dask.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dc9319",
   "metadata": {},
   "source": [
    "Dask is a parallel computing library that can run on various computer resources, from a single machine to a cluster of machines. Here are some common computer resources that can be used with Dask:\n",
    "\n",
    "Single machine: Dask can be run on a single machine, utilizing all the available CPU cores for parallel computing. This is suitable for smaller datasets that can be processed on a single machine.\n",
    "\n",
    "Multi-machine cluster: Dask can also be run on a cluster of machines, allowing for even more parallelism and distributed computing. This is suitable for larger datasets that require more computing power than a single machine can provide.\n",
    "\n",
    "Cloud-based resources: Dask can be run on various cloud-based resources, such as Amazon Web Services (AWS), Microsoft Azure, and Google Cloud Platform (GCP). Cloud-based resources provide flexibility and scalability, allowing users to easily scale up or down their computing resources based on their needs.\n",
    "\n",
    "GPUs: Dask can also utilize GPUs for parallel computing, which can provide significant performance gains for certain types of data processing tasks, such as deep learning.\n",
    "\n",
    "To visualize these computer resources, imagine a diagram with four concentric circles, where the innermost circle represents a single machine, and the outermost circle represents cloud-based resources. The second circle represents a multi-machine cluster, and the third circle represents GPUs. Dask can run on any of these resources, and the size and complexity of the "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e235a28",
   "metadata": {},
   "source": [
    "#### Note that Dask has several optional dependencies that provide additional functionality. You can install these dependencies by running pip install dask[<extra>], where <extra> is the name of the optional dependency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce7870c",
   "metadata": {},
   "source": [
    "### to install the distributed extra, which provides support for distributed computing, you can run pip install dask[graphviz]."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d680c3e",
   "metadata": {},
   "source": [
    "Here are some of the commonly used optional dependencies:\n",
    "\n",
    "numpy: provides support for parallelizing NumPy arrays using Dask.\n",
    "\n",
    "pandas: provides support for parallelizing Pandas dataframes using Dask.\n",
    "\n",
    "scikit-learn: provides support for parallelizing machine learning algorithms using Dask.\n",
    "\n",
    "distributed: provides support for distributed computing using Dask, allowing computations to be executed across multiple machines.\n",
    "\n",
    "matplotlib: provides support for parallelizing Matplotlib plots using Dask.\n",
    "\n",
    "bokeh: provides support for interactive visualizations of Dask computations in web browsers.\n",
    "\n",
    "fsspec: provides support for working with different file systems and data sources using Dask.\n",
    "\n",
    "blosc: provides support for compressing and decompressing data using the Blosc compression library.\n",
    "\n",
    "lz4: provides support for compressing and decompressing data using the LZ4 compression algorithm.\n",
    "\n",
    "zstd: provides support for compressing and decompressing data using the Zstandard compression algorithm."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0f6bd5",
   "metadata": {},
   "source": [
    "### DASK ARRAY"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f261b4f",
   "metadata": {},
   "source": [
    "Dask arrays provide a way to work with large arrays that do not fit into memory by breaking them up into smaller, more manageable chunks and distributing the computation across multiple CPU cores or machines. Dask arrays use lazy evaluation, which means that computations are only executed when the results are needed, allowing for efficient use of system resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "96bda101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dask.array<random_sample, shape=(1000, 1000, 1000), dtype=float64, chunksize=(100, 100, 100), chunktype=numpy.ndarray>\n",
      "(1000, 1000)\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "\n",
    "# create a random array with shape (1000, 1000)\n",
    "x = da.random.random((1000, 1000,1000), chunks=(100, 100,100))\n",
    "print(x)\n",
    "# compute the mean along the first axis\n",
    "y = x.mean(axis=0)\n",
    "\n",
    "# compute the result and print it\n",
    "print(y.compute().shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15d6de65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table>\n",
       "    <tr>\n",
       "        <td>\n",
       "            <table style=\"border-collapse: collapse;\">\n",
       "                <thead>\n",
       "                    <tr>\n",
       "                        <td> </td>\n",
       "                        <th> Array </th>\n",
       "                        <th> Chunk </th>\n",
       "                    </tr>\n",
       "                </thead>\n",
       "                <tbody>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Bytes </th>\n",
       "                        <td> 7.45 GiB </td>\n",
       "                        <td> 126.51 MiB </td>\n",
       "                    </tr>\n",
       "                    \n",
       "                    <tr>\n",
       "                        <th> Shape </th>\n",
       "                        <td> (1000, 1000, 1000) </td>\n",
       "                        <td> (255, 255, 255) </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Dask graph </th>\n",
       "                        <td colspan=\"2\"> 64 chunks in 1 graph layer </td>\n",
       "                    </tr>\n",
       "                    <tr>\n",
       "                        <th> Data type </th>\n",
       "                        <td colspan=\"2\"> float64 numpy.ndarray </td>\n",
       "                    </tr>\n",
       "                </tbody>\n",
       "            </table>\n",
       "        </td>\n",
       "        <td>\n",
       "        <svg width=\"250\" height=\"240\" style=\"stroke:rgb(0,0,0);stroke-width:1\" >\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"10\" y1=\"30\" x2=\"80\" y2=\"101\" />\n",
       "  <line x1=\"10\" y1=\"61\" x2=\"80\" y2=\"131\" />\n",
       "  <line x1=\"10\" y1=\"91\" x2=\"80\" y2=\"162\" />\n",
       "  <line x1=\"10\" y1=\"120\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"10\" y2=\"120\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"28\" y1=\"18\" x2=\"28\" y2=\"138\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"46\" y2=\"156\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"64\" y2=\"174\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 80.58823529411765,70.58823529411765 80.58823529411765,190.58823529411765 10.0,120.0\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"130\" y2=\"0\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"28\" y1=\"18\" x2=\"148\" y2=\"18\" />\n",
       "  <line x1=\"46\" y1=\"36\" x2=\"166\" y2=\"36\" />\n",
       "  <line x1=\"64\" y1=\"54\" x2=\"184\" y2=\"54\" />\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"10\" y1=\"0\" x2=\"80\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"40\" y1=\"0\" x2=\"111\" y2=\"70\" />\n",
       "  <line x1=\"71\" y1=\"0\" x2=\"141\" y2=\"70\" />\n",
       "  <line x1=\"101\" y1=\"0\" x2=\"172\" y2=\"70\" />\n",
       "  <line x1=\"130\" y1=\"0\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"10.0,0.0 130.0,0.0 200.58823529411765,70.58823529411765 80.58823529411765,70.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Horizontal lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"200\" y2=\"70\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"80\" y1=\"101\" x2=\"200\" y2=\"101\" />\n",
       "  <line x1=\"80\" y1=\"131\" x2=\"200\" y2=\"131\" />\n",
       "  <line x1=\"80\" y1=\"162\" x2=\"200\" y2=\"162\" />\n",
       "  <line x1=\"80\" y1=\"190\" x2=\"200\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Vertical lines -->\n",
       "  <line x1=\"80\" y1=\"70\" x2=\"80\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "  <line x1=\"111\" y1=\"70\" x2=\"111\" y2=\"190\" />\n",
       "  <line x1=\"141\" y1=\"70\" x2=\"141\" y2=\"190\" />\n",
       "  <line x1=\"172\" y1=\"70\" x2=\"172\" y2=\"190\" />\n",
       "  <line x1=\"200\" y1=\"70\" x2=\"200\" y2=\"190\" style=\"stroke-width:2\" />\n",
       "\n",
       "  <!-- Colored Rectangle -->\n",
       "  <polygon points=\"80.58823529411765,70.58823529411765 200.58823529411765,70.58823529411765 200.58823529411765,190.58823529411765 80.58823529411765,190.58823529411765\" style=\"fill:#ECB172A0;stroke-width:0\"/>\n",
       "\n",
       "  <!-- Text -->\n",
       "  <text x=\"140.588235\" y=\"210.588235\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" >1000</text>\n",
       "  <text x=\"220.588235\" y=\"130.588235\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(-90,220.588235,130.588235)\">1000</text>\n",
       "  <text x=\"35.294118\" y=\"175.294118\" font-size=\"1.0rem\" font-weight=\"100\" text-anchor=\"middle\" transform=\"rotate(45,35.294118,175.294118)\">1000</text>\n",
       "</svg>\n",
       "        </td>\n",
       "    </tr>\n",
       "</table>"
      ],
      "text/plain": [
       "dask.array<random_sample, shape=(1000, 1000, 1000), dtype=float64, chunksize=(255, 255, 255), chunktype=numpy.ndarray>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import dask.array as da\n",
    "x = da.random.random((1000, 1000,1000))\n",
    "x                     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa68d996",
   "metadata": {},
   "source": [
    "In this example, we create a random Dask array with shape (1000, 1000) and chunk size (100, 100), meaning that the array is broken up into 100x100 chunks. We then compute the mean along the first axis, which returns a Dask array representing the result. Finally, we compute the result using the compute() method and print it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f0226f",
   "metadata": {},
   "source": [
    "### Dask Bag "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ad0b27",
   "metadata": {},
   "source": [
    "Dask Bag is a high-level collection that represents a parallelizable unordered set of Python objects. It is part of the Dask library and is designed to handle large datasets that do not fit in memory. Dask Bag is built on top of Dask's task scheduler and can scale out to clusters of computers.\n",
    "\n",
    "A Dask Bag can be thought of as a distributed version of Python's built-in map function. It can be used to process large datasets in parallel by distributing the processing across multiple cores or nodes in a cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe403f1d",
   "metadata": {},
   "source": [
    " is a parallel and distributed version of Python's built-in map and filter functions. It is designed for working with unstructured or semi-structured data, such as JSON or text files, where the data is not organized into columns and rows like a tabl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3c4f61",
   "metadata": {},
   "source": [
    "### dask data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adbdff77",
   "metadata": {},
   "source": [
    "Dask DataFrame is designed to work with the Dask distributed scheduler, which allows it to scale out computations to a cluster of machines. This makes it well-suited for working with large datasets that would not fit in memory on a single machine. Additionally, Dask DataFrame can leverage parallelism within a single machine, making it faster than Pandas for many operations on large datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17e535e7",
   "metadata": {},
   "source": [
    "#### Dask DataFrame is designed for working with structured data that can be organized into columns and rows like a table, while Dask Bag is designed for working with unstructured or semi-structured data that cannot be easily organized into a table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bdd1ab81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 24.32 s\n"
     ]
    }
   ],
   "source": [
    "import dask\n",
    "import dask.array as da\n",
    "import dask.diagnostics\n",
    "\n",
    "# create a large Dask array\n",
    "x = da.random.normal(size=(100000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# compute the mean of the array\n",
    "with dask.diagnostics.ProgressBar():\n",
    "    result = x.mean().compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c19b402",
   "metadata": {},
   "source": [
    "#### the resource usage of a Dask application"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c4dc175d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 973.26 ms\n",
      "[ResourceData(time=1113950.4061022, mem=202.596352, cpu=0.0)]\n"
     ]
    }
   ],
   "source": [
    "import dask.array as da\n",
    "import dask.diagnostics as diag\n",
    "\n",
    "# Create a large random array\n",
    "x = da.random.random((10000, 10000), chunks=(1000, 1000))\n",
    "\n",
    "# Compute the mean along each row\n",
    "y = x.mean(axis=1)\n",
    "\n",
    "# Monitor resource usage during computation\n",
    "with diag.ProgressBar(), diag.ResourceProfiler(dt=0.25) as prof:\n",
    "    y.compute()\n",
    "\n",
    "# Print resource usage statistics\n",
    "print(prof.results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d2a459a",
   "metadata": {},
   "source": [
    "time: the total amount of CPU time used by the Dask application so far (in seconds).\n",
    "\n",
    "mem: the amount of memory used by the Dask application (in MB).\n",
    "\n",
    "cpu: the fraction of CPU time used by the Dask application, as a percentage of the total available CPU time.\n",
    "\n",
    "These values can be useful in monitoring the resource usage of a Dask application and identifying any potential bottlenecks or areas for optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed3a3d3",
   "metadata": {},
   "source": [
    "###  parallel computing "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e8af8c2",
   "metadata": {},
   "source": [
    "Dask is a library for parallel computing in Python, which allows users to execute computations in parallel across multiple CPU cores, multiple machines, or even on GPUs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "62ec8850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "result = add(1, 2)\n",
    "\n",
    "from dask import delayed\n",
    "\n",
    "@delayed\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "result = add(1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "94177c0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99001260-0ab9-4b96-b4a4-df82190f48f7",
   "metadata": {},
   "source": [
    "Dask[distributed] extends the functionality of Dask by providing a distributed computing scheduler, which allows Dask to execute tasks across multiple machines in a cluster. This makes it possible to handle even larger data sets and more complex workflows than with Dask alone.\n",
    "\n",
    "The distributed scheduler of Dask[distributed] uses a client-server model, where a single central scheduler coordinates the execution of tasks across multiple worker nodes. This allows the scheduler to balance the workload across the nodes, and to handle failures and recoveries gracefully.\n",
    "\n",
    "Dask[distributed] provides a familiar programming interface for users already familiar with Dask, as well as additional features such as real-time progress updates, dynamic workload balancing, and support for complex workflows involving multiple steps and data dependencies. It is a popular choice for distributed computing in the Python ecosystem, particularly in the context of big data processing and machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fe73bc-b498-48d3-a0e1-7ca4fd0c51f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "85581192-370b-4b76-b5ae-3116da569e8e",
   "metadata": {},
   "source": [
    "Dask[distributed] can also be used in a local machine, which means that it can be used to parallelize tasks across multiple CPU cores on a single machine. This can be particularly useful for speeding up data processing workflows that involve large datasets or complex computations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c0f221d-f582-4330-a669-5c17343be2f8",
   "metadata": {},
   "source": [
    "To use Dask[distributed] in a local machine, you can install the dask and dask[distributed] libraries using pip,"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4dcd1ce-dc08-41e3-ac7d-947b5a5ae156",
   "metadata": {},
   "source": [
    "Modern CPUs typically have multiple cores, which allows them to perform multiple tasks simultaneously and improve overall performance. For example, a quad-core CPU has four cores, while an octa-core CPU has eight cores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b79fa50-ac42-49ca-b306-e87cc591e197",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CPU cores: 4\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "num_cores = multiprocessing.cpu_count()\n",
    "\n",
    "print(\"Number of CPU cores:\", num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8766700f-69ab-4948-b4b2-d162d6c81ca4",
   "metadata": {},
   "source": [
    "This creates a local cluster with one worker per CPU core, and a client that can be used to submit tasks to the cluster for parallel execution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8711936-75aa-4b51-9e72-e31e04d4602e",
   "metadata": {},
   "source": [
    "The jupyterlab_dash module provides an App class that you can use to create a new Dask Dashboard app inside a JupyterLab tab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "892f7b93",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DaskDashboard' from 'jupyterlab_dash' (C:\\Users\\MY PC\\miniconda3\\envs\\hypothisis_test\\lib\\site-packages\\jupyterlab_dash\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [1], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[0;32m      5\u001b[0m client \u001b[38;5;241m=\u001b[39m Client()\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mjupyterlab_dash\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DaskDashboard\n\u001b[0;32m      9\u001b[0m dashboard \u001b[38;5;241m=\u001b[39m DaskDashboard()\n\u001b[0;32m     10\u001b[0m dashboard\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'DaskDashboard' from 'jupyterlab_dash' (C:\\Users\\MY PC\\miniconda3\\envs\\hypothisis_test\\lib\\site-packages\\jupyterlab_dash\\__init__.py)"
     ]
    }
   ],
   "source": [
    "import jupyterlab_dash\n",
    "\n",
    "# create a Dask Dashboard\n",
    "from dask.distributed import Client\n",
    "client = Client()\n",
    "\n",
    "from jupyterlab_dash import DaskDashboard\n",
    "\n",
    "dashboard = DaskDashboard()\n",
    "dashboard.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14c74032-c065-41ff-bbce-44250e7f8a44",
   "metadata": {},
   "source": [
    "This creates a local cluster with one worker per CPU core, and a client that can be used to submit tasks to the cluster for parallel execution. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0ef85a2-e97c-4105-9f83-bb58c660b5fa",
   "metadata": {},
   "source": [
    "#### use 4  core of your local machine  useing dask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1905fc44-229a-4c14-94ae-7ad70d6b5bc2",
   "metadata": {},
   "source": [
    "To use 4 cores of your local machine with Dask, you can create a Dask Client object that connects to a LocalCluster with 4 workers, each running on a separate core."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3511ba7b-c91d-4655-aa82-2877bee3d71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "04b705fd-95a5-4260-9ea2-4050b3e8df7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4999960042.737427\n"
     ]
    }
   ],
   "source": [
    "from dask.distributed import Client, LocalCluster\n",
    "import dask.array as da\n",
    "\n",
    "# create a Dask cluster with 4 workers\n",
    "cluster = LocalCluster(n_workers=4, threads_per_worker=1)\n",
    "client = Client(cluster)\n",
    "\n",
    "# create a large Dask array\n",
    "x = da.random.random((100000, 100000), chunks=(1000, 1000))\n",
    "\n",
    "# compute the sum of the array in parallel\n",
    "result = x.sum()\n",
    "\n",
    "# print the result\n",
    "print(result.compute())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c9334-342b-463b-abb8-ad172872faa9",
   "metadata": {},
   "source": [
    "##### Dask allows you to parallelize your Python code across multiple cores, but it does not directly provide a way to assign specific tasks to specific cores. Instead, Dask automatically distributes tasks across available cores and workers in a way that maximizes efficiency and minimizes communication overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11cc2a-3928-4309-895c-bcdcb3e0a1ed",
   "metadata": {},
   "source": [
    "you can use the Client object in Dask to submit tasks to different workers, which can be run on different cores. Here's an example of how to submit different tasks to different workers using Dask:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3cc12-cf46-4b78-967e-581c1ae5b67f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Start a Dask client\n",
    "client = Client()\n",
    "\n",
    "# Define your functions\n",
    "def function1():\n",
    "    # do something\n",
    "    pass\n",
    "\n",
    "def function2():\n",
    "    # do something else\n",
    "    pass\n",
    "\n",
    "# Submit the functions to different workers\n",
    "future1 = client.submit(function1, workers=['worker1'])\n",
    "future2 = client.submit(function2, workers=['worker2'])\n",
    "\n",
    "# Wait for the results\n",
    "result1 = future1.result()\n",
    "result2 = future2.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f5e4d-472e-40c1-8845-eba73d4cd25a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
