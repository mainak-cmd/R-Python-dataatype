{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "995b74c3",
   "metadata": {},
   "source": [
    "How do you generate random numbers in NumPy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7978b8",
   "metadata": {},
   "source": [
    "Random numbers in NumPy can be generated using the \"numpy.random\" module. For example, to generate a random array of shape (3, 3) with values between 0 and 1, you can use the command \"numpy.random.rand(3, 3)\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecd61f85",
   "metadata": {},
   "source": [
    "What is the purpose of universal functions in NumPy?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060b4c4",
   "metadata": {},
   "source": [
    "Universal functions, or ufuncs, are functions that operate element-wise on NumPy arrays and can perform a wide range of mathematical operations. They are optimized for speed and efficiency and can significantly improve the performance of numerical computations in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "fb6a46a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5 7 3]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Create two arrays of different sizes\n",
    "x = np.array([1, 2, 3])\n",
    "y = np.array([4, 5,0])\n",
    "\n",
    "# Add the two arrays using the np.add() ufunc\n",
    "z = np.add(x, y)\n",
    "\n",
    "print(z) # Output: [5 7 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d4973",
   "metadata": {},
   "source": [
    "What is the difference between a shallow copy and a deep copy in NumPy?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80cfddc2",
   "metadata": {},
   "source": [
    "\n",
    "A shallow copy creates a new array object that shares the same data as the original array, while a deep copy creates a new array object with a completely new copy of the data. A shallow copy is more memory-efficient but can lead to unintended changes to the original array, while a deep copy is safer but can be more memory-intensive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94aea20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "How do you handle missing or null values in NumPy arrays?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af41ffb2",
   "metadata": {},
   "source": [
    "NumPy arrays can handle missing or null values using the NaN (not a number) or None values. These values can be replaced or removed using NumPy functions such as isnan(), where(), and nan_to_num()."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "637c7527",
   "metadata": {},
   "source": [
    "### BGV Chabot and MMM: NLP Approach"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b82c2e",
   "metadata": {},
   "source": [
    "Developing and deploying a BGV\n",
    "Chabot on teams to ease the\n",
    "procedure, minimize the total\n",
    "Operation cost, and cut it down \n",
    "to8%."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9deb8c9a",
   "metadata": {},
   "source": [
    "Find out the nessity of promotion of a particular brand useing its popularity index based on the reviews and stars useing sentiment analysis (Sentiment lexicon) and convert then into flot datatype (Sentiment scoring) that we use as a Popularity index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baebc762",
   "metadata": {},
   "source": [
    "Popularity index refers to a numerical value or ranking that is used to represent the level of popularity of a particular item or entity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf28793",
   "metadata": {},
   "source": [
    "assume that promotion is linearly dependent on Popularity index so we create a new categgorical feature name promotion useing bin technique in feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89dbfa7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age age_range\n",
      "0   18       low\n",
      "1   22    medium\n",
      "2   25    medium\n",
      "3   28    medium\n",
      "4   31      high\n",
      "5   35      high\n",
      "6   38      high\n",
      "7   40      high\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Create a list of ages\n",
    "ages = [18, 22, 25, 28, 31, 35, 38, 40]\n",
    "\n",
    "# Define the bins for the age ranges\n",
    "bins = [0, 20, 30, 100]\n",
    "\n",
    "# Create a new column for the age ranges\n",
    "df = pd.DataFrame({'age': ages})\n",
    "df['age_range'] = pd.cut(df['age'], bins=bins, labels=['low', 'medium', 'high'])\n",
    "\n",
    "# Print the result\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6482ee4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<20', '20-30', '20-30', '20-30', '30+', '30+', '30+', '30+']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# Create a list of ages\n",
    "ages = [18, 22, 25, 28, 31, 35, 38, 40]\n",
    "\n",
    "# Define the bins for the age ranges\n",
    "bins = [0, 20, 30, 100]\n",
    "\n",
    "# Convert ages to numpy array\n",
    "ages_np = np.array(ages)\n",
    "\n",
    "# Create a new array for the age ranges\n",
    "age_ranges = np.digitize(ages_np, bins=bins, right=False)\n",
    "\n",
    "# Map the bin indices to the age range categories\n",
    "categories = ['<20', '20-30', '30+']\n",
    "age_categories = [categories[i-1] for i in age_ranges]\n",
    "\n",
    "# Print the result\n",
    "print(age_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61d8261e",
   "metadata": {},
   "source": [
    "Find the right combination of\n",
    "products, prices, promotions, and\n",
    "distribution (place) so that a\n",
    "The company can gain and\n",
    "maintainFuture Stocks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca525672",
   "metadata": {},
   "source": [
    "Virtual\n",
    "Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedab0e1",
   "metadata": {},
   "source": [
    "Can you explain the difference between rule-based and statistical NLP, and when you might use one approach over the other?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07a94263",
   "metadata": {},
   "source": [
    "Rule-based NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7412efe2",
   "metadata": {},
   "source": [
    "Rule-based NLP is an approach that relies on a set of predefined rules to analyze and understand text. These rules are often created by linguists and language experts, and they can be used to identify parts of speech, syntax, and other linguistic features of text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e41b1b",
   "metadata": {},
   "source": [
    "Statistical NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e0d416f",
   "metadata": {},
   "source": [
    "Statistical NLP, on the other hand, uses machine learning algorithms to analyze large amounts of text data and learn patterns and relationships between different linguistic features. This approach can be used to automatically identify topics, sentiment, and other features of text, without relying on predefined rules."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4854bcb",
   "metadata": {},
   "source": [
    "Ultimately, the choice between rule-based and statistical NLP will depend on the specific use case and the requirements of the project. For example, if you're working with a small amount of text data and need a high level of accuracy, rule-based NLP might be the better choice. But if you're dealing with large amounts of data and need to quickly identify patterns and relationships, statistical NLP might be the better approach."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45ccdfc3",
   "metadata": {},
   "source": [
    "### sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bb3d54",
   "metadata": {},
   "source": [
    "The goal of sentiment analysis is to determine the overall attitude or opinion expressed in the text, whether it's positive, negative, or neutral."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a1ed06",
   "metadata": {},
   "source": [
    "Data collection: Collect text data from various sources such as social media, customer reviews, or news articles.\n",
    "\n",
    "Preprocessing: Clean and preprocess the text data by removing noise, stop words, and special characters. This step may also involve stemming or lemmatization to reduce words to their root form.\n",
    "\n",
    "Sentiment lexicon creation: Create a sentiment lexicon or dictionary containing a list of words or phrases with their associated sentiment scores (positive, negative, or neutral).\n",
    "\n",
    "Sentiment scoring: Assign a sentiment score to each word or phrase in the text using the sentiment lexicon. The sentiment score can be a binary value (positive or negative) or a continuous value on a scale from -1 to 1.\n",
    "\n",
    "Aggregation: Combine the sentiment scores of individual words or phrases to obtain an overall sentiment score for the text. This can be done by taking the average score, summing the scores, or using other aggregation methods.\n",
    "\n",
    "Visualization and interpretation: Visualize the sentiment score using charts or graphs to help understand the overall sentiment of the text. Interpret the sentiment score and identify key topics or themes in the text that are driving the sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47e5567",
   "metadata": {},
   "outputs": [],
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43342a17",
   "metadata": {},
   "source": [
    "Tokenization: This stage involves splitting the text into individual words, phrases, or sentences. The goal is to break down the text into smaller units that can be processed more easily. Tokenization can be done using a simple whitespace tokenizer or more advanced methods such as regular expressions or machine learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7ba6c03b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'a', 'sample', 'sentence', 'It', 'contains', 'multiple', 'words', 'and', 'punctuations']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"This is a sample sentence. It contains multiple words and punctuations!\"\n",
    "\n",
    "# Use regular expression to split text into tokens\n",
    "tokens = re.findall(r'\\b\\w+\\b', text)\n",
    "\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a380ea85",
   "metadata": {},
   "source": [
    "#### r'\\b\\w+\\b'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f734eba7",
   "metadata": {},
   "source": [
    "\\b\\w+\\b is a regular expression pattern that matches one or more word characters (\\w+) surrounded by word boundaries (\\b). Word characters are any alphanumeric characters ([a-zA-Z0-9_]), so this pattern will match any sequence of one or more consecutive letters, numbers, or underscores that appear as individual \"words\" in a text."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b70586e1",
   "metadata": {},
   "source": [
    "### regular expressions in python"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "259ed177",
   "metadata": {},
   "source": [
    " powerful tools for searching, replacing, and manipulating text. Regular expressions are essentially patterns that define a set of strings that match the pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96943218",
   "metadata": {},
   "outputs": [],
   "source": [
    "Matching a pattern: You can use the re.match() function to check if a string matches a given pattern."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "74ad6814",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text matches the pattern\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"Hello, world!\"\n",
    "\n",
    "# Check if text matches the pattern \"Hello\"\n",
    "match = re.match(r'Hello', text)\n",
    "\n",
    "if match:\n",
    "    print(\"Text matches the pattern\")\n",
    "else:\n",
    "    print(\"Text does not match the pattern\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c8222b",
   "metadata": {},
   "outputs": [],
   "source": [
    "Searching for a pattern: You can use the re.search() function to search for a pattern in a string. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5d9fa20e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pattern found at position 10\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "# Search for the pattern \"brown\"\n",
    "match = re.search(r'brown', text)\n",
    "\n",
    "if match:\n",
    "    print(\"Pattern found at position\", match.start())\n",
    "else:\n",
    "    print(\"Pattern not found\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da434037",
   "metadata": {},
   "source": [
    "Replacing a pattern: You can use the re.sub() function to replace a pattern with a new string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a6c8374",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The quick brown cat jumps over the lazy dog.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "\n",
    "#Replace the word \"fox\" with \"cat\"\n",
    "new_text = re.sub(r'fox', 'cat', text)\n",
    "\n",
    "print(new_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1299cc10",
   "metadata": {},
   "source": [
    "### Lowercasing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcb0c068",
   "metadata": {},
   "source": [
    "This stage involves converting all text to lowercase or uppercase to reduce the number of unique words and simplify the analysis. This is particularly useful when dealing with case-insensitive tasks such as sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b41f2ccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "this is a sample text.\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a Sample Text.\"\n",
    "\n",
    "lower_text = text.lower()\n",
    "\n",
    "print(lower_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022bb2b6",
   "metadata": {},
   "source": [
    "### stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da41892d",
   "metadata": {},
   "source": [
    " stop words are words that are commonly used in a language and do not carry much meaning on their own, such as \"the\", \"and\", \"is\", \"at\", etc. These words are usually removed from text data during preprocessing to reduce the dimensionality of the data and improve the performance of models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74ab893e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'sample', 'sentence', 'containing', 'stop', 'words', '.']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "text = \"This is a sample sentence containing stop words.\"\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "filtered_tokens = [token for token in tokens if not token in stop_words]\n",
    "\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b0e648",
   "metadata": {},
   "source": [
    "### Stemming and Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13bd5695",
   "metadata": {},
   "source": [
    "Stemming  These stages involve reducing words to their base or root form to normalize the text and group similar words together. Stemming involves removing suffixes and prefixes from words to derive the stem, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ca48c9e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'troubl', 'troubl', 'troubl', 'run', 'runner', 'ran', 'run', 'fairli', 'easili', 'fair']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Example words to be stemmed\n",
    "words = ['cats', 'trouble', 'troubling', 'troubled', 'running', 'runner', 'ran', 'runs', 'fairly', 'easily', 'fairness']\n",
    "\n",
    "# Stemming the words\n",
    "stemmed_words = [stemmer.stem(word) for word in words]\n",
    "\n",
    "print(stemmed_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fca2f2",
   "metadata": {},
   "source": [
    "while lemmatization involves using a dictionary or database to map words to their base forms. For example, \"running\", \"runs\", and \"ran\" can be stemmed to \"run\", while \"am\", \"are\", and \"is\" can be lemmatized to \"be\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c147be0f",
   "metadata": {},
   "source": [
    " WordNetLemmatizer can also take a second argument to specify the POS of the word, which can help in cases where the word has multiple meanings or can be used as different parts of speech."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb67742d",
   "metadata": {},
   "source": [
    "In the context of NLP, the POS of a word is often used in tasks such as text classification, information retrieval, and sentiment analysis. For example, in sentiment analysis, the POS of a word can help determine if a word is being used in a positive or negative context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "59e54e5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['cat', 'trouble', 'trouble', 'trouble', 'run', 'runner', 'run', 'run', 'fairly', 'easily', 'fairness']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Example words to be lemmatized\n",
    "words = ['cats', 'trouble', 'troubling', 'troubled', 'running', 'runner', 'ran', 'runs', 'fairly', 'easily', 'fairness']\n",
    "\n",
    "# Lemmatizing the words\n",
    "lemmatized_words = [lemmatizer.lemmatize(word,pos='v') for word in words]\n",
    "\n",
    "print(lemmatized_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e9df5ac",
   "metadata": {},
   "source": [
    "###  Normalization and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2665ce3e",
   "metadata": {},
   "source": [
    "Normalization refers to the process of converting text data to a standardized format. This can include converting all text to lowercase or uppercase, removing punctuation and special characters, and replacing contractions with their full forms. The goal of normalization is to reduce the variability in the text data and make it easier to analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5ee80c",
   "metadata": {},
   "source": [
    "Cleaning refers to the process of removing unwanted or irrelevant elements from text data. This can include removing HTML tags, URLs, email addresses, and other types of noise that do not contribute to the meaning of the text. Cleaning can also involve removing stop words, which are commonly used words that do not carry much meaning on their own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4a89a129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample text punctuation contractions fuck visit information\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "text = \"This is a Sample Text, with some punctuation and contractions and fuck off. Visit https://example.com for more information.\"\n",
    "stop_words = set(stopwords.words('english'))\n",
    "# Normalize text\n",
    "text = text.lower()  # Convert to lowercase\n",
    "text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "text = re.sub(r'\\s+', ' ', text)  # Remove extra whitespace\n",
    "text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "text = re.sub(r'\\b\\w{1,3}\\b', '', text)  # Remove short words\n",
    "\n",
    "# Clean text\n",
    "text = re.sub(r'<.*?>', '', text)  # Remove HTML tags\n",
    "text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "text = re.sub(r'\\S+@\\S+', '', text)  # Remove email addresses\n",
    "text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stop words\n",
    "\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09507f70",
   "metadata": {},
   "source": [
    "Sentiment lexicon creation in Python involves creating a dictionary or list of words or phrases with their associated sentiment scores. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4ea2bb01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import nltk\n",
    "from nltk.corpus import opinion_lexicon\n",
    "\n",
    "# Define function to create sentiment lexicon\n",
    "def create_sentiment_lexicon():\n",
    "    # Load positive and negative opinion lexicons from NLTK\n",
    "    pos_lexicon = opinion_lexicon.positive()\n",
    "    neg_lexicon = opinion_lexicon.negative()\n",
    "\n",
    "    # Create an empty dictionary to store sentiment scores\n",
    "    sentiment_lexicon = {}\n",
    "\n",
    "    # Assign positive sentiment score of +1 to positive words\n",
    "    for word in pos_lexicon:\n",
    "        sentiment_lexicon[word] = 1\n",
    "\n",
    "    # Assign negative sentiment score of -1 to negative words\n",
    "    for word in neg_lexicon:\n",
    "        sentiment_lexicon[word] = -1\n",
    "\n",
    "    return sentiment_lexicon\n",
    "\n",
    "# Call the function to create the sentiment lexicon\n",
    "lexicon = create_sentiment_lexicon()\n",
    "\n",
    "# Print the sentiment score of a word from the lexicon\n",
    "print(lexicon.get('happy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0852bfc5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
